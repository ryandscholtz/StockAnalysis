"""
Large PDF Processing System for Financial Documents
Handles 160+ page documents efficiently with chunking, batching, and progress tracking
"""
import os
import logging
import json
import asyncio
from typing import Dict, List, Optional, Any, Tuple, Callable
import boto3
from botocore.exceptions import ClientError
import io
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
import math

logger = logging.getLogger(__name__)


class LargePDFProcessor:
    """
    Advanced PDF processor for large financial documents (160+ pages)
    Features:
    - Intelligent chunking and batching
    - AWS Textract with S3 async processing for large docs
    - Page-by-page LLM processing with context management
    - Progress tracking and resumable processing
    - Memory-efficient streaming
    """

    def __init__(self):
        self.aws_region = os.getenv("AWS_REGION", "eu-west-1")
        self.aws_profile = os.getenv("AWS_PROFILE", "Cerebrum")
        self.s3_bucket = os.getenv("TEXTRACT_S3_BUCKET", "stock-analysis-textract-temp")
        
        # Initialize AWS clients
        try:
            self.session = boto3.Session(profile_name=self.aws_profile, region_name=self.aws_region)
            self.textract = self.session.client('textract')
            self.s3 = self.session.client('s3')
            logger.info(f"LargePDFProcessor initialized (region: {self.aws_region})")
        except Exception as e:
            logger.error(f"Failed to initialize AWS clients: {e}")
            raise

        # Processing limits
        self.MAX_SYNC_PAGES = 10  # Use sync Textract for small docs
        self.MAX_ASYNC_PAGES = 3000  # Textract async limit
        self.CHUNK_SIZE_PAGES = 20  # Process in chunks for LLM
        self.MAX_CONTEXT_CHARS = 150000  # Conservative LLM context limit

    async def process_large_pdf(
        self, 
        pdf_bytes: bytes, 
        ticker: str,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Tuple[Dict[str, Any], str]:
        """
        Process large PDF with intelligent strategy selection
        
        Args:
            pdf_bytes: PDF file bytes
            ticker: Stock ticker symbol
            progress_callback: Optional callback(current_step, total_steps, status_message)
            
        Returns:
            Tuple of (structured_financial_data, processing_summary)
        """
        try:
            # Step 1: Analyze PDF to determine processing strategy
            page_count = self._get_pdf_page_count(pdf_bytes)
            pdf_size_mb = len(pdf_bytes) / (1024 * 1024)
            
            logger.info(f"Processing PDF for {ticker}: {page_count} pages, {pdf_size_mb:.1f} MB")
            
            if progress_callback:
                progress_callback(1, 10, f"Analyzing PDF: {page_count} pages, {pdf_size_mb:.1f} MB")
            
            # Step 2: Choose processing strategy based on size
            if page_count <= self.MAX_SYNC_PAGES:
                logger.info("Using FAST processing: Sync Textract + Single LLM call")
                return await self._process_small_pdf(pdf_bytes, ticker, progress_callback)
            
            elif page_count <= self.MAX_ASYNC_PAGES:
                logger.info("Using SCALABLE processing: Async Textract + Chunked LLM")
                return await self._process_large_pdf_async(pdf_bytes, ticker, progress_callback)
            
            else:
                logger.info("Using BATCH processing: Split PDF + Parallel processing")
                return await self._process_huge_pdf_batched(pdf_bytes, ticker, progress_callback)
                
        except Exception as e:
            logger.error(f"Error processing large PDF for {ticker}: {e}")
            if progress_callback:
                progress_callback(10, 10, f"Error: {str(e)}")
            raise

    async def _process_small_pdf(
        self, 
        pdf_bytes: bytes, 
        ticker: str,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Tuple[Dict[str, Any], str]:
        """Process small PDFs (â‰¤10 pages) with sync Textract"""
        
        if progress_callback:
            progress_callback(2, 10, "Extracting text with AWS Textract (sync)")
        
        # Use sync Textract
        response = self.textract.analyze_document(
            Document={'Bytes': pdf_bytes},
            FeatureTypes=['TABLES', 'FORMS']
        )
        
        # Extract text and tables
        text = self._extract_text_from_response(response)
        tables = self._extract_tables_from_response(response)
        
        if progress_callback:
            progress_callback(6, 10, f"Extracted {len(text)} chars, {len(tables)} tables")
        
        # Single LLM call for small documents
        if progress_callback:
            progress_callback(8, 10, "Structuring data with LLM")
        
        structured_data = await self._structure_data_with_llm(text, tables, ticker)
        
        if progress_callback:
            progress_callback(10, 10, "Processing complete")
        
        summary = f"Small PDF processed: {len(text)} chars extracted, {len(tables)} tables"
        return structured_data, summary

    async def _process_large_pdf_async(
        self, 
        pdf_bytes: bytes, 
        ticker: str,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Tuple[Dict[str, Any], str]:
        """Process large PDFs (11-3000 pages) with async Textract"""
        
        # Step 1: Upload to S3 for async processing
        if progress_callback:
            progress_callback(2, 10, "Uploading PDF to S3 for async processing")
        
        s3_key = f"textract-temp/{ticker}-{int(time.time())}.pdf"
        self.s3.put_object(
            Bucket=self.s3_bucket,
            Key=s3_key,
            Body=pdf_bytes,
            ContentType='application/pdf'
        )
        
        try:
            # Step 2: Start async Textract job
            if progress_callback:
                progress_callback(3, 10, "Starting AWS Textract async job")
            
            response = self.textract.start_document_analysis(
                DocumentLocation={
                    'S3Object': {
                        'Bucket': self.s3_bucket,
                        'Name': s3_key
                    }
                },
                FeatureTypes=['TABLES', 'FORMS']
            )
            
            job_id = response['JobId']
            logger.info(f"Started Textract job {job_id} for {ticker}")
            
            # Step 3: Wait for completion with progress updates
            if progress_callback:
                progress_callback(4, 10, f"Processing with Textract (Job: {job_id})")
            
            text, tables = await self._wait_for_textract_job(job_id, progress_callback)
            
            # Step 4: Process in chunks for LLM
            if progress_callback:
                progress_callback(7, 10, f"Structuring data in chunks ({len(text)} chars)")
            
            structured_data = await self._process_text_in_chunks(text, tables, ticker, progress_callback)
            
            if progress_callback:
                progress_callback(10, 10, "Large PDF processing complete")
            
            summary = f"Large PDF processed: {len(text)} chars, {len(tables)} tables, chunked processing"
            return structured_data, summary
            
        finally:
            # Cleanup S3 object
            try:
                self.s3.delete_object(Bucket=self.s3_bucket, Key=s3_key)
            except Exception as e:
                logger.warning(f"Failed to cleanup S3 object {s3_key}: {e}")

    async def _process_huge_pdf_batched(
        self, 
        pdf_bytes: bytes, 
        ticker: str,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Tuple[Dict[str, Any], str]:
        """Process huge PDFs (3000+ pages) by splitting into batches"""
        
        if progress_callback:
            progress_callback(2, 10, "Splitting huge PDF into processable batches")
        
        # Split PDF into smaller chunks
        pdf_chunks = await self._split_pdf_into_chunks(pdf_bytes, max_pages_per_chunk=2000)
        
        logger.info(f"Split huge PDF into {len(pdf_chunks)} chunks for processing")
        
        # Process chunks in parallel
        if progress_callback:
            progress_callback(4, 10, f"Processing {len(pdf_chunks)} chunks in parallel")
        
        chunk_results = []
        with ThreadPoolExecutor(max_workers=3) as executor:  # Limit concurrent Textract jobs
            futures = []
            
            for i, chunk_bytes in enumerate(pdf_chunks):
                future = executor.submit(
                    self._process_pdf_chunk_sync, 
                    chunk_bytes, 
                    f"{ticker}-chunk-{i+1}"
                )
                futures.append(future)
            
            # Collect results as they complete
            for i, future in enumerate(as_completed(futures)):
                try:
                    chunk_result = future.result()
                    chunk_results.append(chunk_result)
                    
                    if progress_callback:
                        progress_callback(
                            4 + (3 * (i + 1) // len(futures)), 
                            10, 
                            f"Processed chunk {i+1}/{len(pdf_chunks)}"
                        )
                        
                except Exception as e:
                    logger.error(f"Error processing chunk {i}: {e}")
                    # Continue with other chunks
        
        # Merge results from all chunks
        if progress_callback:
            progress_callback(8, 10, f"Merging results from {len(chunk_results)} chunks")
        
        merged_data = self._merge_chunk_results(chunk_results)
        
        if progress_callback:
            progress_callback(10, 10, "Huge PDF processing complete")
        
        summary = f"Huge PDF processed: {len(pdf_chunks)} chunks, {len(chunk_results)} successful"
        return merged_data, summary

    def _get_pdf_page_count(self, pdf_bytes: bytes) -> int:
        """Get PDF page count efficiently"""
        try:
            import PyPDF2
            pdf_file = io.BytesIO(pdf_bytes)
            pdf_reader = PyPDF2.PdfReader(pdf_file)
            return len(pdf_reader.pages)
        except Exception:
            # Fallback: estimate from file size (rough approximation)
            size_mb = len(pdf_bytes) / (1024 * 1024)
            estimated_pages = max(1, int(size_mb * 10))  # ~10 pages per MB
            logger.warning(f"Could not get exact page count, estimating {estimated_pages} pages")
            return estimated_pages

    async def _wait_for_textract_job(
        self, 
        job_id: str, 
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Tuple[str, List[Dict]]:
        """Wait for async Textract job to complete and retrieve results"""
        
        max_wait_time = 3600  # 1 hour max
        check_interval = 30  # Check every 30 seconds
        elapsed_time = 0
        
        while elapsed_time < max_wait_time:
            response = self.textract.get_document_analysis(JobId=job_id)
            status = response['JobStatus']
            
            if status == 'SUCCEEDED':
                logger.info(f"Textract job {job_id} completed successfully")
                
                # Get all pages of results
                all_blocks = []
                next_token = None
                
                while True:
                    if next_token:
                        response = self.textract.get_document_analysis(
                            JobId=job_id, 
                            NextToken=next_token
                        )
                    
                    all_blocks.extend(response.get('Blocks', []))
                    next_token = response.get('NextToken')
                    
                    if not next_token:
                        break
                
                # Extract text and tables
                text = self._extract_text_from_blocks(all_blocks)
                tables = self._extract_tables_from_blocks(all_blocks)
                
                return text, tables
                
            elif status == 'FAILED':
                error_msg = f"Textract job {job_id} failed"
                logger.error(error_msg)
                raise Exception(error_msg)
                
            elif status in ['IN_PROGRESS', 'PARTIAL_SUCCESS']:
                if progress_callback:
                    progress_callback(
                        5, 10, 
                        f"Textract processing... ({elapsed_time}s elapsed)"
                    )
                
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
            else:
                logger.warning(f"Unknown Textract job status: {status}")
                await asyncio.sleep(check_interval)
                elapsed_time += check_interval
        
        raise Exception(f"Textract job {job_id} timed out after {max_wait_time} seconds")

    async def _process_text_in_chunks(
        self, 
        text: str, 
        tables: List[Dict], 
        ticker: str,
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> Dict[str, Any]:
        """Process large text in chunks to handle LLM context limits"""
        
        # Split text into manageable chunks
        chunks = self._split_text_into_chunks(text, self.MAX_CONTEXT_CHARS)
        logger.info(f"Split text into {len(chunks)} chunks for LLM processing")
        
        # Process chunks in parallel
        chunk_results = []
        
        # Use semaphore to limit concurrent LLM calls
        semaphore = asyncio.Semaphore(3)  # Max 3 concurrent LLM calls
        
        async def process_chunk(chunk_text: str, chunk_idx: int) -> Dict[str, Any]:
            async with semaphore:
                try:
                    return await self._structure_data_with_llm(chunk_text, [], ticker)
                except Exception as e:
                    logger.error(f"Error processing chunk {chunk_idx}: {e}")
                    return {"income_statement": {}, "balance_sheet": {}, "cashflow": {}, "key_metrics": {}}
        
        # Process all chunks
        tasks = [
            process_chunk(chunk, i) 
            for i, chunk in enumerate(chunks)
        ]
        
        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter out exceptions and merge results
        valid_results = [
            result for result in chunk_results 
            if isinstance(result, dict)
        ]
        
        logger.info(f"Successfully processed {len(valid_results)}/{len(chunks)} chunks")
        
        # Merge all chunk results
        merged_data = self._merge_financial_data(valid_results)
        
        return merged_data

    def _split_text_into_chunks(self, text: str, max_chars: int) -> List[str]:
        """Split text into chunks while preserving context"""
        if len(text) <= max_chars:
            return [text]
        
        chunks = []
        current_pos = 0
        
        while current_pos < len(text):
            # Find a good break point (end of paragraph or sentence)
            end_pos = min(current_pos + max_chars, len(text))
            
            if end_pos < len(text):
                # Look for paragraph break
                para_break = text.rfind('\n\n', current_pos, end_pos)
                if para_break > current_pos:
                    end_pos = para_break
                else:
                    # Look for sentence break
                    sent_break = text.rfind('. ', current_pos, end_pos)
                    if sent_break > current_pos:
                        end_pos = sent_break + 1
            
            chunk = text[current_pos:end_pos].strip()
            if chunk:
                chunks.append(chunk)
            
            current_pos = end_pos
        
        return chunks

    async def _structure_data_with_llm(
        self, 
        text: str, 
        tables: List[Dict], 
        ticker: str
    ) -> Dict[str, Any]:
        """Structure extracted text/tables into financial data using LLM"""
        
        # This would integrate with your existing LLM processing
        # For now, return a basic structure
        logger.info(f"Structuring {len(text)} chars of text with LLM for {ticker}")
        
        # TODO: Integrate with existing LLM extraction logic
        # This is where you'd call your Llama/OpenAI vision models
        
        return {
            "income_statement": {},
            "balance_sheet": {},
            "cashflow": {},
            "key_metrics": {}
        }

    def _merge_financial_data(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge financial data from multiple chunks/sources"""
        merged = {
            "income_statement": {},
            "balance_sheet": {},
            "cashflow": {},
            "key_metrics": {}
        }
        
        for result in results:
            # Merge income statements
            for period, data in result.get("income_statement", {}).items():
                if period not in merged["income_statement"]:
                    merged["income_statement"][period] = {}
                merged["income_statement"][period].update(data)
            
            # Merge balance sheets
            for period, data in result.get("balance_sheet", {}).items():
                if period not in merged["balance_sheet"]:
                    merged["balance_sheet"][period] = {}
                merged["balance_sheet"][period].update(data)
            
            # Merge cash flows
            for period, data in result.get("cashflow", {}).items():
                if period not in merged["cashflow"]:
                    merged["cashflow"][period] = {}
                merged["cashflow"][period].update(data)
            
            # Merge key metrics (take most recent/complete)
            merged["key_metrics"].update(result.get("key_metrics", {}))
        
        return merged

    # Helper methods for text/table extraction
    def _extract_text_from_response(self, response: Dict) -> str:
        """Extract text from Textract response"""
        text_blocks = []
        for block in response.get('Blocks', []):
            if block['BlockType'] == 'LINE':
                text_blocks.append(block.get('Text', ''))
        return '\n'.join(text_blocks)

    def _extract_text_from_blocks(self, blocks: List[Dict]) -> str:
        """Extract text from Textract blocks"""
        text_blocks = []
        for block in blocks:
            if block['BlockType'] == 'LINE':
                text_blocks.append(block.get('Text', ''))
        return '\n'.join(text_blocks)

    def _extract_tables_from_response(self, response: Dict) -> List[Dict]:
        """Extract tables from Textract response"""
        # Implementation similar to existing TextractExtractor
        return []

    def _extract_tables_from_blocks(self, blocks: List[Dict]) -> List[Dict]:
        """Extract tables from Textract blocks"""
        # Implementation similar to existing TextractExtractor
        return []

    async def _split_pdf_into_chunks(self, pdf_bytes: bytes, max_pages_per_chunk: int) -> List[bytes]:
        """Split PDF into smaller chunks"""
        # This would use PyPDF2 to split the PDF
        # For now, return the original PDF as a single chunk
        return [pdf_bytes]

    def _process_pdf_chunk_sync(self, chunk_bytes: bytes, chunk_name: str) -> Dict[str, Any]:
        """Process a PDF chunk synchronously"""
        # This would process a single chunk
        return {
            "income_statement": {},
            "balance_sheet": {},
            "cashflow": {},
            "key_metrics": {}
        }

    def _merge_chunk_results(self, chunk_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Merge results from multiple PDF chunks"""
        return self._merge_financial_data(chunk_results)